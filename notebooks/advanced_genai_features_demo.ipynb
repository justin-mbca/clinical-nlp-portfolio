{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a7c2ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Hugging Face tokenizer parallelism and common warnings for cleaner output\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474e5f7d",
   "metadata": {},
   "source": [
    "# Advanced GenAI Features Demo\n",
    "This notebook demonstrates advanced GenAI, NLP, and LLM features for recruiter-ready healthcare AI/data science portfolios.\n",
    "\n",
    "**Features Demonstrated:**\n",
    "- Entity extraction and classification with BERT/Bio_ClinicalBERT (Hugging Face Transformers)\n",
    "- Retrieval-Augmented Generation (RAG) pipeline\n",
    "- Vector database integration (FAISS/Chroma)\n",
    "- Prompt engineering and finetuning\n",
    "- Bias detection, model guardrails, and safety checks\n",
    "- Cloud integration (AWS, S3, cloud ML workflows)\n",
    "- PEFT/SFT advanced finetuning (Hugging Face PEFT)\n",
    "Each section includes code, workflow explanation, and practical tips for production and portfolio use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e0934e",
   "metadata": {},
   "source": [
    "## 1. Entity Extraction & Classification with Transformers\n",
    "This section demonstrates how to use BERT/Bio_ClinicalBERT and Hugging Face Transformers for entity extraction and classification in clinical text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "88a8df02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note 1: Patient reports chest pain and shortness of breath. History of hypertension.\n",
      "  Entity: of | Type: DISEASE\n",
      "\n",
      "Note 2: Diabetic patient with fatigue and nausea. No chest pain.\n",
      "  Entity: tic | Type: DISEASE\n",
      "  Entity: fatigue | Type: DISEASE\n",
      "  Entity: nausea | Type: DISEASE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Load Bio_ClinicalBERT model and tokenizer\n",
    "model_checkpoint = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=5)  # Example: 5 labels\n",
    "\n",
    "labels = ['O', 'B-DISEASE', 'I-DISEASE', 'B-SYMPTOM', 'I-SYMPTOM']\n",
    "\n",
    "def get_entities(text, model, tokenizer, labels):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)[0].tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    for token, pred in zip(tokens, predictions):\n",
    "        label = labels[pred]\n",
    "        if label.startswith('B-'):\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "            current_entity = {'entity': label[2:], 'text': token.replace('##', '')}\n",
    "        elif label.startswith('I-') and current_entity:\n",
    "            current_entity['text'] += token.replace('##', '')\n",
    "        else:\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = None\n",
    "    if current_entity:\n",
    "        entities.append(current_entity)\n",
    "    return entities\n",
    "\n",
    "# Example clinical notes\n",
    "notes = [\n",
    "    'Patient reports chest pain and shortness of breath. History of hypertension.',\n",
    "    'Diabetic patient with fatigue and nausea. No chest pain.'\n",
    " ]\n",
    "\n",
    "for i, note in enumerate(notes):\n",
    "    ents = get_entities(note, model, tokenizer, labels)\n",
    "    print(f'Note {i+1}:', note)\n",
    "    for ent in ents:\n",
    "        print(f\"  Entity: {ent['text']} | Type: {ent['entity']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd18fda",
   "metadata": {},
   "source": [
    "## 2. Retrieval-Augmented Generation (RAG) Pipeline\n",
    "This section demonstrates a simple RAG pipeline using local models and custom retrievers/generators for clinical QA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "38e2aca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the diagnosis for patient 123?\n",
      "Retrieved Document: Patient 123 has diabetes and hypertension.\n",
      "Generated Answer: LLM answer based on: Patient 123 has diabetes and hypertension.\n"
     ]
    }
   ],
   "source": [
    "# Simple RAG pipeline demo\n",
    "def simple_retriever(query, docs):\n",
    "    # Return the most relevant document (here, just the first for demo)\n",
    "    return docs[0]\n",
    "\n",
    "def simple_generator(text):\n",
    "    # Simulate LLM answer generation\n",
    "    return f\"LLM answer based on: {text}\"\n",
    "\n",
    "# Example documents and query\n",
    "documents = [\n",
    "    \"Patient 123 has diabetes and hypertension.\",\n",
    "    \"Patient 456 has asthma and no history of diabetes.\"\n",
    " ]\n",
    "query = \"What is the diagnosis for patient 123?\"\n",
    "\n",
    "# RAG workflow\n",
    "retrieved_doc = simple_retriever(query, documents)\n",
    "generated_answer = simple_generator(retrieved_doc)\n",
    "print(\"Query:\", query)\n",
    "print(\"Retrieved Document:\", retrieved_doc)\n",
    "print(\"Generated Answer:\", generated_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11700fe",
   "metadata": {},
   "source": [
    "## 3. Vector Database Integration (FAISS)\n",
    "This section demonstrates how to use FAISS for semantic search and retrieval in clinical NLP workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfd2fff",
   "metadata": {},
   "source": [
    "### Alternative: Vector Database Integration with Annoy\n",
    "FAISS is not currently supported on Python 3.13. Annoy is a pure Python library for approximate nearest neighbor search and works with the latest Python versions. Below is a demo using Annoy for semantic search in clinical NLP workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "851df4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding: [0.15, 0.15]\n",
      "Top 2 nearest indices: [0, 1]\n",
      "Distances: [0.0707106739282608, 0.0707106813788414]\n"
     ]
    }
   ],
   "source": [
    "# Annoy vector search demo (works with Python 3.13)\n",
    "# Install Annoy if not already installed\n",
    "import sys\n",
    "try:\n",
    "    from annoy import AnnoyIndex\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'annoy'])\n",
    "    from annoy import AnnoyIndex\n",
    "import numpy as np\n",
    "\n",
    "# Create example embeddings (2D for demo)\n",
    "embeddings = np.array([[0.1, 0.2], [0.2, 0.1], [0.9, 0.8]], dtype='float32')\n",
    "f = embeddings.shape[1]\n",
    "index = AnnoyIndex(f, 'euclidean')\n",
    "for i, vec in enumerate(embeddings):\n",
    "    index.add_item(i, vec)\n",
    "index.build(10)  # 10 trees\n",
    "\n",
    "# Query embedding\n",
    "query_embedding = [0.15, 0.15]\n",
    "nearest_indices = index.get_nns_by_vector(query_embedding, 2, include_distances=True)\n",
    "print(\"Query embedding:\", query_embedding)\n",
    "print(\"Top 2 nearest indices:\", nearest_indices[0])\n",
    "print(\"Distances:\", nearest_indices[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4314e1bb",
   "metadata": {},
   "source": [
    "## 4. Prompt Engineering and Finetuning\n",
    "This section demonstrates prompt engineering and basic finetuning techniques using Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f651a466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The patient was diagnosed with [MASK].\n",
      "Prediction: cancer | Score: 0.5427\n",
      "Prediction: leukemia | Score: 0.0917\n",
      "Prediction: schizophrenia | Score: 0.0470\n",
      "\n",
      "Finetuning: Use Trainer API with your labeled dataset for supervised training.\n"
     ]
    }
   ],
   "source": [
    "# Prompt engineering demo with Hugging Face Transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "# Use a fill-mask pipeline for prompt engineering\n",
    "fill_mask = pipeline('fill-mask', model='bert-base-uncased')\n",
    "prompt = \"The patient was diagnosed with [MASK].\"\n",
    "results = fill_mask(prompt)\n",
    "print(\"Prompt:\", prompt)\n",
    "for result in results[:3]:\n",
    "    print(f\"Prediction: {result['token_str']} | Score: {result['score']:.4f}\")\n",
    "\n",
    "# Finetuning demo (conceptual, not executed)\n",
    "print(\"\\nFinetuning: Use Trainer API with your labeled dataset for supervised training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf32c4",
   "metadata": {},
   "source": [
    "## 5. Bias Detection, Model Guardrails, and Safety Checks\n",
    "This section demonstrates basic bias detection and safety checks for NLP models using Python and scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e67c005e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00         6\n",
      "   macro avg       1.00      1.00      1.00         6\n",
      "weighted avg       1.00      1.00      1.00         6\n",
      "\n",
      "Group 2 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75         3\n",
      "           1       1.00      0.33      0.50         3\n",
      "\n",
      "    accuracy                           0.67         6\n",
      "   macro avg       0.80      0.67      0.62         6\n",
      "weighted avg       0.80      0.67      0.62         6\n",
      "\n",
      "Accuracy Group 1: 1.00\n",
      "Accuracy Group 2: 0.67\n",
      "Warning: Potential bias detected between groups!\n"
     ]
    }
   ],
   "source": [
    "# Bias detection and safety checks demo\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Example predictions and true labels for two groups\n",
    "y_true = np.array([1, 0, 1, 0, 1, 0])  # 1: Disease, 0: No Disease\n",
    "y_pred_group1 = np.array([1, 0, 1, 0, 1, 0])  # Group 1 predictions\n",
    "y_pred_group2 = np.array([0, 0, 1, 0, 0, 0])  # Group 2 predictions\n",
    "\n",
    "print(\"Group 1 Classification Report:\")\n",
    "print(classification_report(y_true, y_pred_group1))\n",
    "\n",
    "print(\"Group 2 Classification Report:\")\n",
    "print(classification_report(y_true, y_pred_group2))\n",
    "\n",
    "# Simple bias check: Compare accuracy between groups\n",
    "acc_group1 = np.mean(y_true == y_pred_group1)\n",
    "acc_group2 = np.mean(y_true == y_pred_group2)\n",
    "print(f\"Accuracy Group 1: {acc_group1:.2f}\")\n",
    "print(f\"Accuracy Group 2: {acc_group2:.2f}\")\n",
    "if abs(acc_group1 - acc_group2) > 0.2:\n",
    "    print(\"Warning: Potential bias detected between groups!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7e2bd8",
   "metadata": {},
   "source": [
    "## 6. Cloud Integration (AWS, S3, Cloud ML Workflows)\n",
    "This section demonstrates how to integrate with cloud platforms for data storage, model deployment, and ML workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "136c6543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For full cloud ML workflows, use AWS SageMaker for training/deployment.\n"
     ]
    }
   ],
   "source": [
    "# Cloud integration demo: Upload file to AWS S3 (requires AWS credentials)\n",
    "import boto3\n",
    "\n",
    "# Example: Upload a file to S3 (conceptual, not executed)\n",
    "def upload_to_s3(file_path, bucket, object_name):\n",
    "    s3 = boto3.client('s3')\n",
    "    try:\n",
    "        s3.upload_file(file_path, bucket, object_name)\n",
    "        print(f\"Uploaded {file_path} to s3://{bucket}/{object_name}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error uploading to S3:\", e)\n",
    "\n",
    "# Example usage (commented out)\n",
    "# upload_to_s3('model.pt', 'my-ml-bucket', 'models/model.pt')\n",
    "\n",
    "print(\"For full cloud ML workflows, use AWS SageMaker for training/deployment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e134f27e",
   "metadata": {},
   "source": [
    "## 7. PEFT/SFT Advanced Finetuning\n",
    "This section demonstrates parameter-efficient finetuning (PEFT/SFT) using Hugging Face PEFT library for LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "021c47db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT model ready for parameter-efficient finetuning.\n",
      "For full training, use Trainer API with your labeled dataset.\n"
     ]
    }
   ],
   "source": [
    "# PEFT/SFT advanced finetuning demo (conceptual)\n",
    "# Requires: pip install peft transformers datasets\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load base model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Configure LoRA (Low-Rank Adaptation) for PEFT\n",
    "lora_config = LoraConfig(r=8, lora_alpha=32, target_modules=[\"query\", \"value\"], lora_dropout=0.1)\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"PEFT model ready for parameter-efficient finetuning.\")\n",
    "print(\"For full training, use Trainer API with your labeled dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aacada",
   "metadata": {},
   "source": [
    "## Load Synthetic Clinical Notes\n",
    "Read the synthetic clinical notes CSV and display the first few entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d86d0da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Patient reports fatigue and thirst.</td>\n",
       "      <td>Diabetes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Patient reports wheezing and shortness of breath.</td>\n",
       "      <td>Asthma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Patient reports thirst and frequent urination.</td>\n",
       "      <td>Diabetes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Patient reports healthy and no complaints.</td>\n",
       "      <td>No Disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Patient reports wheezing and cough.</td>\n",
       "      <td>Asthma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       label\n",
       "0                Patient reports fatigue and thirst.    Diabetes\n",
       "1  Patient reports wheezing and shortness of breath.      Asthma\n",
       "2     Patient reports thirst and frequent urination.    Diabetes\n",
       "3         Patient reports healthy and no complaints.  No Disease\n",
       "4                Patient reports wheezing and cough.      Asthma"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "notes_df = pd.read_csv('../notebooks/synthetic_clinical_notes.csv')\n",
    "notes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "373ee361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "63b5ab43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected clinical note from column 'text':\n",
      "Patient reports fatigue and thirst.\n",
      "GenAI pipeline result:\n",
      "{'length': 35, 'preview': 'Patient reports fatigue and thirst.'}\n"
     ]
    }
   ],
   "source": [
    "# Select the first clinical note for processing\n",
    "clinical_note = notes_df.iloc[0]['text']\n",
    "print(\"Selected clinical note from column 'text':\")\n",
    "print(clinical_note)\n",
    "\n",
    "# Example: Pass the note to your GenAI pipeline (replace with your actual function)\n",
    "from nlp_pipeline import process_note\n",
    "result = process_note(clinical_note)\n",
    "print('GenAI pipeline result:')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7cad9f",
   "metadata": {},
   "source": [
    "## Advanced NLP Entity Extraction Demo\n",
    "This cell demonstrates how to use spaCy and Hugging Face Transformers for named entity recognition and clinical entity extraction on a sample clinical note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5ad3ebc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note length: 76\n",
      "Preview: Patient reports chest pain and shortness of breath\n",
      "spaCy Named Entities: No spaCy entities found or error occurred\n",
      "Clinical Entities (Bio_ClinicalBERT): No clinical entities found or error occurred\n"
     ]
    }
   ],
   "source": [
    "# Example clinical note for advanced NLP demo\n",
    "clinical_note = \"Patient reports chest pain and shortness of breath. History of hypertension.\"\n",
    "\n",
    "from nlp_pipeline import process_note\n",
    "result = process_note(clinical_note)\n",
    "\n",
    "print(\"Note length:\", result.get('length'))\n",
    "print(\"Preview:\", result.get('preview'))\n",
    "print(\"spaCy Named Entities:\", result.get('spacy_entities', 'No spaCy entities found or error occurred'))\n",
    "print(\"Clinical Entities (Bio_ClinicalBERT):\", result.get('clinical_entities', 'No clinical entities found or error occurred'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51daa64",
   "metadata": {},
   "source": [
    "## Advanced NLP Entity Extraction: Multiple Test Cases\n",
    "Try several clinical notes to see how spaCy and Bio_ClinicalBERT extract entities from different types of medical text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "83305650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Case 1:\n",
      "Note: John Doe, a 45-year-old male, was diagnosed with diabetes on 2022-03-15.\n",
      "Note length: 72\n",
      "Preview: John Doe, a 45-year-old male, was diagnosed with d\n",
      "spaCy Named Entities: No spaCy entities found or error occurred\n",
      "Clinical Entities (Bio_ClinicalBERT): No clinical entities found or error occurred\n",
      "\n",
      "Test Case 2:\n",
      "Note: Patient presents with severe headache and nausea. MRI scheduled for 10/10/2025.\n",
      "Note length: 79\n",
      "Preview: Patient presents with severe headache and nausea. \n",
      "spaCy Named Entities: No spaCy entities found or error occurred\n",
      "Clinical Entities (Bio_ClinicalBERT): No clinical entities found or error occurred\n",
      "\n",
      "Test Case 3:\n",
      "Note: Jane Smith has a history of hypertension and COPD. Prescribed Lisinopril.\n",
      "Note length: 73\n",
      "Preview: Jane Smith has a history of hypertension and COPD.\n",
      "spaCy Named Entities: No spaCy entities found or error occurred\n",
      "Clinical Entities (Bio_ClinicalBERT): No clinical entities found or error occurred\n",
      "\n",
      "Test Case 4:\n",
      "Note: Patient reports chest pain, shortness of breath, and cough. No prior cardiac history.\n",
      "Note length: 85\n",
      "Preview: Patient reports chest pain, shortness of breath, a\n",
      "spaCy Named Entities: No spaCy entities found or error occurred\n",
      "Clinical Entities (Bio_ClinicalBERT): No clinical entities found or error occurred\n",
      "\n",
      "Test Case 5:\n",
      "Note: Michael Johnson, DOB 1970-05-22, admitted for acute asthma exacerbation.\n",
      "Note length: 72\n",
      "Preview: Michael Johnson, DOB 1970-05-22, admitted for acut\n",
      "spaCy Named Entities: No spaCy entities found or error occurred\n",
      "Clinical Entities (Bio_ClinicalBERT): No clinical entities found or error occurred\n"
     ]
    }
   ],
   "source": [
    "# Multiple clinical note test cases for advanced NLP entity extraction\n",
    "test_notes = [\n",
    "    \"John Doe, a 45-year-old male, was diagnosed with diabetes on 2022-03-15.\",\n",
    "    \"Patient presents with severe headache and nausea. MRI scheduled for 10/10/2025.\",\n",
    "    \"Jane Smith has a history of hypertension and COPD. Prescribed Lisinopril.\",\n",
    "    \"Patient reports chest pain, shortness of breath, and cough. No prior cardiac history.\",\n",
    "    \"Michael Johnson, DOB 1970-05-22, admitted for acute asthma exacerbation.\"\n",
    "]\n",
    "\n",
    "from nlp_pipeline import process_note\n",
    "\n",
    "for i, note in enumerate(test_notes):\n",
    "    print(f\"\\nTest Case {i+1}:\")\n",
    "    print(\"Note:\", note)\n",
    "    result = process_note(note)\n",
    "    print(\"Note length:\", result.get('length'))\n",
    "    print(\"Preview:\", result.get('preview'))\n",
    "    print(\"spaCy Named Entities:\", result.get('spacy_entities', 'No spaCy entities found or error occurred'))\n",
    "    print(\"Clinical Entities (Bio_ClinicalBERT):\", result.get('clinical_entities', 'No clinical entities found or error occurred'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51819a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biomedical NER Results:\n",
      "\n",
      "Test Case 1:\n",
      "Note: John Doe, a 45-year-old male, was diagnosed with diabetes on 2022-03-15.\n",
      "  Entity: 45 - year - old | Type: Age | Score: 0.99\n",
      "  Entity: diabetes | Type: Disease_disorder | Score: 1.00\n",
      "  Entity: 03 | Type: Date | Score: 1.00\n",
      "\n",
      "Test Case 2:\n",
      "Note: Patient presents with severe headache and nausea. MRI scheduled for 10/10/2025.\n",
      "  Entity: severe | Type: Severity | Score: 1.00\n",
      "  Entity: headache | Type: Sign_symptom | Score: 1.00\n",
      "  Entity: nausea | Type: Sign_symptom | Score: 1.00\n",
      "  Entity: mri | Type: Diagnostic_procedure | Score: 1.00\n",
      "\n",
      "Test Case 3:\n",
      "Note: Jane Smith has a history of hypertension and COPD. Prescribed Lisinopril.\n",
      "  Entity: hypertension | Type: History | Score: 0.98\n",
      "  Entity: cop | Type: History | Score: 0.99\n",
      "  Entity: li | Type: Medication | Score: 1.00\n",
      "  Entity: ##sin | Type: Medication | Score: 0.96\n",
      "  Entity: ##op | Type: Medication | Score: 1.00\n",
      "\n",
      "Test Case 4:\n",
      "Note: Patient reports chest pain, shortness of breath, and cough. No prior cardiac history.\n",
      "  Entity: chest | Type: Biological_structure | Score: 1.00\n",
      "  Entity: shortness of breath | Type: Sign_symptom | Score: 0.96\n",
      "  Entity: no | Type: History | Score: 0.98\n",
      "\n",
      "Test Case 5:\n",
      "Note: Michael Johnson, DOB 1970-05-22, admitted for acute asthma exacerbation.\n",
      "  Entity: 05 | Type: Date | Score: 0.90\n",
      "  Entity: acute | Type: Detailed_description | Score: 1.00\n",
      "  Entity: asthma | Type: Disease_disorder | Score: 0.94\n"
     ]
    }
   ],
   "source": [
    "# Biomedical NER with Hugging Face Transformers (d4data/biomedical-ner-all)\n",
    "# This cell demonstrates advanced biomedical entity extraction for clinical notes.\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the biomedical NER pipeline\n",
    "biomed_ner = pipeline('ner', model='d4data/biomedical-ner-all', aggregation_strategy='simple')\n",
    "\n",
    "print('Biomedical NER Results:')\n",
    "for i, note in enumerate(test_notes):\n",
    "    print(f\"\\nTest Case {i+1}:\")\n",
    "    print(\"Note:\", note)\n",
    "    entities = biomed_ner(note)\n",
    "    if entities:\n",
    "        for ent in entities:\n",
    "            print(f\"  Entity: {ent['word']} | Type: {ent['entity_group']} | Score: {ent['score']:.2f}\")\n",
    "    else:\n",
    "        print(\"  No biomedical entities found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5375740e",
   "metadata": {},
   "source": [
    "## MCP Demo: Store and Retrieve Agent Context\n",
    "This cell demonstrates how to use Model Context Protocol (MCP) to store and retrieve analysis results for agentic workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "39058f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved context for ClinicalAgent:\n",
      "[{'length': 72, 'preview': 'Michael Johnson, DOB 1970-05-22, admitted for acut'}]\n"
     ]
    }
   ],
   "source": [
    "from agents.mcp import ModelContext\n",
    "\n",
    "# Create ModelContext object\n",
    "context = ModelContext()\n",
    "\n",
    "# Store the GenAI pipeline result for the agent\n",
    "context.update_context('ClinicalAgent', result)\n",
    "\n",
    "# Retrieve and display the context for the agent\n",
    "retrieved = context.get_context('ClinicalAgent')\n",
    "print('Retrieved context for ClinicalAgent:')\n",
    "print(retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033c62d8",
   "metadata": {},
   "source": [
    "## Analysis Overview: Synthetic Clinical Notes with GenAI Features\n",
    "This section demonstrates how advanced GenAI features are applied to synthetic clinical notes. The workflow includes:\n",
    "- Loading and inspecting synthetic clinical notes data.\n",
    "- Selecting a clinical note for analysis.\n",
    "- Processing the note using an NLP pipeline powered by large language models (LLMs).\n",
    "- Using Retrieval-Augmented Generation (RAG) to fetch relevant medical codes and guidelines based on extracted entities.\n",
    "- Leveraging Model Context Protocol (MCP) to maintain conversational memory and context for multi-turn agent interactions.\n",
    "This analysis showcases how clinical AI agents can extract key information, ground their responses in external knowledge, and remember context for follow-up questions, supporting dynamic and explainable healthcare NLP solutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
