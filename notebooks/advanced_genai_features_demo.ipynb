{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c2ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Hugging Face tokenizer parallelism and common warnings for cleaner output\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474e5f7d",
   "metadata": {},
   "source": [
    "# Advanced GenAI Features Demo\n",
    "This notebook demonstrates advanced GenAI, NLP, and LLM features for recruiter-ready healthcare AI/data science portfolios.\n",
    "\n",
    "**Features Demonstrated:**\n",
    "- Entity extraction and classification with BERT/Bio_ClinicalBERT (Hugging Face Transformers)\n",
    "- Retrieval-Augmented Generation (RAG) pipeline\n",
    "- Vector database integration (FAISS/Chroma)\n",
    "- Prompt engineering and finetuning\n",
    "- Bias detection, model guardrails, and safety checks\n",
    "- Cloud integration (AWS, S3, cloud ML workflows)\n",
    "- PEFT/SFT advanced finetuning (Hugging Face PEFT)\n",
    "Each section includes code, workflow explanation, and practical tips for production and portfolio use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e0934e",
   "metadata": {},
   "source": [
    "## 1. Entity Extraction & Classification with Transformers\n",
    "This section demonstrates how to use BERT/Bio_ClinicalBERT and Hugging Face Transformers for entity extraction and classification in clinical text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a8df02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justin/nlp_clincal/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note 1: Patient reports chest pain and shortness of breath. History of hypertension.\n",
      "  Entity: [CLS] | Type: DISEASE\n",
      "  Entity: patient | Type: DISEASE\n",
      "  Entity: reports | Type: SYMPTOM\n",
      "  Entity: andshort | Type: SYMPTOM\n",
      "  Entity: ness | Type: SYMPTOM\n",
      "  Entity: of | Type: DISEASE\n",
      "  Entity: breath | Type: DISEASE\n",
      "  Entity: . | Type: DISEASE\n",
      "  Entity: history | Type: DISEASE\n",
      "  Entity: of | Type: DISEASE\n",
      "  Entity: h | Type: DISEASE\n",
      "  Entity: yper | Type: DISEASE\n",
      "  Entity: ion | Type: DISEASE\n",
      "  Entity: .[SEP] | Type: DISEASE\n",
      "\n",
      "Note 2: Diabetic patient with fatigue and nausea. No chest pain.\n",
      "  Entity: [CLS] | Type: DISEASE\n",
      "  Entity: abe | Type: DISEASE\n",
      "  Entity: tic | Type: DISEASE\n",
      "  Entity: patient | Type: DISEASE\n",
      "  Entity: with | Type: DISEASE\n",
      "  Entity: fatigue | Type: DISEASE\n",
      "  Entity: and | Type: DISEASE\n",
      "  Entity: nausea | Type: DISEASE\n",
      "  Entity: . | Type: DISEASE\n",
      "  Entity: nochest | Type: SYMPTOM\n",
      "  Entity: pain | Type: SYMPTOM\n",
      "  Entity: .[SEP] | Type: DISEASE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Load Bio_ClinicalBERT model and tokenizer\n",
    "model_checkpoint = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=5)  # Example: 5 labels\n",
    "\n",
    "labels = ['O', 'B-DISEASE', 'I-DISEASE', 'B-SYMPTOM', 'I-SYMPTOM']\n",
    "\n",
    "def get_entities(text, model, tokenizer, labels):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)[0].tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    for token, pred in zip(tokens, predictions):\n",
    "        label = labels[pred]\n",
    "        if label.startswith('B-'):\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "            current_entity = {'entity': label[2:], 'text': token.replace('##', '')}\n",
    "        elif label.startswith('I-') and current_entity:\n",
    "            current_entity['text'] += token.replace('##', '')\n",
    "        else:\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = None\n",
    "    if current_entity:\n",
    "        entities.append(current_entity)\n",
    "    return entities\n",
    "\n",
    "# Example clinical notes\n",
    "notes = [\n",
    "    'Patient reports chest pain and shortness of breath. History of hypertension.',\n",
    "    'Diabetic patient with fatigue and nausea. No chest pain.'\n",
    " ]\n",
    "\n",
    "for i, note in enumerate(notes):\n",
    "    ents = get_entities(note, model, tokenizer, labels)\n",
    "    print(f'Note {i+1}:', note)\n",
    "    for ent in ents:\n",
    "        print(f\"  Entity: {ent['text']} | Type: {ent['entity']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd18fda",
   "metadata": {},
   "source": [
    "## 2. Retrieval-Augmented Generation (RAG) Pipeline\n",
    "This section demonstrates a simple RAG pipeline using local models and custom retrievers/generators for clinical QA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38e2aca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the diagnosis for patient 123?\n",
      "Retrieved Document: Patient 123 has diabetes and hypertension.\n",
      "Generated Answer: LLM answer based on: Patient 123 has diabetes and hypertension.\n"
     ]
    }
   ],
   "source": [
    "# Simple RAG pipeline demo\n",
    "def simple_retriever(query, docs):\n",
    "    # Return the most relevant document (here, just the first for demo)\n",
    "    return docs[0]\n",
    "\n",
    "def simple_generator(text):\n",
    "    # Simulate LLM answer generation\n",
    "    return f\"LLM answer based on: {text}\"\n",
    "\n",
    "# Example documents and query\n",
    "documents = [\n",
    "    \"Patient 123 has diabetes and hypertension.\",\n",
    "    \"Patient 456 has asthma and no history of diabetes.\"\n",
    " ]\n",
    "query = \"What is the diagnosis for patient 123?\"\n",
    "\n",
    "# RAG workflow\n",
    "retrieved_doc = simple_retriever(query, documents)\n",
    "generated_answer = simple_generator(retrieved_doc)\n",
    "print(\"Query:\", query)\n",
    "print(\"Retrieved Document:\", retrieved_doc)\n",
    "print(\"Generated Answer:\", generated_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11700fe",
   "metadata": {},
   "source": [
    "## 3. Vector Database Integration (FAISS)\n",
    "This section demonstrates how to use FAISS for semantic search and retrieval in clinical NLP workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfd2fff",
   "metadata": {},
   "source": [
    "### Alternative: Vector Database Integration with Annoy\n",
    "FAISS is not currently supported on Python 3.13. Annoy is a pure Python library for approximate nearest neighbor search and works with the latest Python versions. Below is a demo using Annoy for semantic search in clinical NLP workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "851df4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting annoy\n",
      "  Downloading annoy-1.17.3.tar.gz (647 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies: started\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: annoy\n",
      "  Building wheel for annoy (pyproject.toml): started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: annoy\n",
      "  Building wheel for annoy (pyproject.toml): started\n",
      "  Building wheel for annoy (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for annoy: filename=annoy-1.17.3-cp313-cp313-macosx_13_0_arm64.whl size=60132 sha256=79fdef27beca9d443e993b25b49003dbf744cab6397ddd39be61351794833a11\n",
      "  Stored in directory: /Users/justin/Library/Caches/pip/wheels/fc/4e/87/f1f957e7382aa370452cff98276f51686924d5415e449800ed\n",
      "Successfully built annoy\n",
      "Installing collected packages: annoy\n",
      "Successfully installed annoy-1.17.3\n",
      "Query embedding: [0.15, 0.15]\n",
      "Top 2 nearest indices: [0, 1]\n",
      "Distances: [0.0707106739282608, 0.0707106813788414]\n",
      "  Building wheel for annoy (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for annoy: filename=annoy-1.17.3-cp313-cp313-macosx_13_0_arm64.whl size=60132 sha256=79fdef27beca9d443e993b25b49003dbf744cab6397ddd39be61351794833a11\n",
      "  Stored in directory: /Users/justin/Library/Caches/pip/wheels/fc/4e/87/f1f957e7382aa370452cff98276f51686924d5415e449800ed\n",
      "Successfully built annoy\n",
      "Installing collected packages: annoy\n",
      "Successfully installed annoy-1.17.3\n",
      "Query embedding: [0.15, 0.15]\n",
      "Top 2 nearest indices: [0, 1]\n",
      "Distances: [0.0707106739282608, 0.0707106813788414]\n"
     ]
    }
   ],
   "source": [
    "# Annoy vector search demo (works with Python 3.13)\n",
    "# Install Annoy if not already installed\n",
    "import sys\n",
    "try:\n",
    "    from annoy import AnnoyIndex\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'annoy'])\n",
    "    from annoy import AnnoyIndex\n",
    "import numpy as np\n",
    "\n",
    "# Create example embeddings (2D for demo)\n",
    "embeddings = np.array([[0.1, 0.2], [0.2, 0.1], [0.9, 0.8]], dtype='float32')\n",
    "f = embeddings.shape[1]\n",
    "index = AnnoyIndex(f, 'euclidean')\n",
    "for i, vec in enumerate(embeddings):\n",
    "    index.add_item(i, vec)\n",
    "index.build(10)  # 10 trees\n",
    "\n",
    "# Query embedding\n",
    "query_embedding = [0.15, 0.15]\n",
    "nearest_indices = index.get_nns_by_vector(query_embedding, 2, include_distances=True)\n",
    "print(\"Query embedding:\", query_embedding)\n",
    "print(\"Top 2 nearest indices:\", nearest_indices[0])\n",
    "print(\"Distances:\", nearest_indices[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4314e1bb",
   "metadata": {},
   "source": [
    "## 4. Prompt Engineering and Finetuning\n",
    "This section demonstrates prompt engineering and basic finetuning techniques using Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f651a466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The patient was diagnosed with [MASK].\n",
      "Prediction: cancer | Score: 0.5427\n",
      "Prediction: leukemia | Score: 0.0917\n",
      "Prediction: schizophrenia | Score: 0.0470\n",
      "\n",
      "Finetuning: Use Trainer API with your labeled dataset for supervised training.\n"
     ]
    }
   ],
   "source": [
    "# Prompt engineering demo with Hugging Face Transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "# Use a fill-mask pipeline for prompt engineering\n",
    "fill_mask = pipeline('fill-mask', model='bert-base-uncased')\n",
    "prompt = \"The patient was diagnosed with [MASK].\"\n",
    "results = fill_mask(prompt)\n",
    "print(\"Prompt:\", prompt)\n",
    "for result in results[:3]:\n",
    "    print(f\"Prediction: {result['token_str']} | Score: {result['score']:.4f}\")\n",
    "\n",
    "# Finetuning demo (conceptual, not executed)\n",
    "print(\"\\nFinetuning: Use Trainer API with your labeled dataset for supervised training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf32c4",
   "metadata": {},
   "source": [
    "## 5. Bias Detection, Model Guardrails, and Safety Checks\n",
    "This section demonstrates basic bias detection and safety checks for NLP models using Python and scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias detection and safety checks demo\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Example predictions and true labels for two groups\n",
    "y_true = np.array([1, 0, 1, 0, 1, 0])  # 1: Disease, 0: No Disease\n",
    "y_pred_group1 = np.array([1, 0, 1, 0, 1, 0])  # Group 1 predictions\n",
    "y_pred_group2 = np.array([0, 0, 1, 0, 0, 0])  # Group 2 predictions\n",
    "\n",
    "print(\"Group 1 Classification Report:\")\n",
    "print(classification_report(y_true, y_pred_group1))\n",
    "\n",
    "print(\"Group 2 Classification Report:\")\n",
    "print(classification_report(y_true, y_pred_group2))\n",
    "\n",
    "# Simple bias check: Compare accuracy between groups\n",
    "acc_group1 = np.mean(y_true == y_pred_group1)\n",
    "acc_group2 = np.mean(y_true == y_pred_group2)\n",
    "print(f\"Accuracy Group 1: {acc_group1:.2f}\")\n",
    "print(f\"Accuracy Group 2: {acc_group2:.2f}\")\n",
    "if abs(acc_group1 - acc_group2) > 0.2:\n",
    "    print(\"Warning: Potential bias detected between groups!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7e2bd8",
   "metadata": {},
   "source": [
    "## 6. Cloud Integration (AWS, S3, Cloud ML Workflows)\n",
    "This section demonstrates how to integrate with cloud platforms for data storage, model deployment, and ML workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136c6543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud integration demo: Upload file to AWS S3 (requires AWS credentials)\n",
    "import boto3\n",
    "\n",
    "# Example: Upload a file to S3 (conceptual, not executed)\n",
    "def upload_to_s3(file_path, bucket, object_name):\n",
    "    s3 = boto3.client('s3')\n",
    "    try:\n",
    "        s3.upload_file(file_path, bucket, object_name)\n",
    "        print(f\"Uploaded {file_path} to s3://{bucket}/{object_name}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error uploading to S3:\", e)\n",
    "\n",
    "# Example usage (commented out)\n",
    "# upload_to_s3('model.pt', 'my-ml-bucket', 'models/model.pt')\n",
    "\n",
    "print(\"For full cloud ML workflows, use AWS SageMaker for training/deployment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e134f27e",
   "metadata": {},
   "source": [
    "## 7. PEFT/SFT Advanced Finetuning\n",
    "This section demonstrates parameter-efficient finetuning (PEFT/SFT) using Hugging Face PEFT library for LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021c47db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT/SFT advanced finetuning demo (conceptual)\n",
    "# Requires: pip install peft transformers datasets\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load base model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Configure LoRA (Low-Rank Adaptation) for PEFT\n",
    "lora_config = LoraConfig(r=8, lora_alpha=32, target_modules=[\"query\", \"value\"], lora_dropout=0.1)\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"PEFT model ready for parameter-efficient finetuning.\")\n",
    "print(\"For full training, use Trainer API with your labeled dataset.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
